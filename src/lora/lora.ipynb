{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA\n",
    "\n",
    "### Notebook to help understand how LoRA works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10cc8b390>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4902,  1.2622, -1.0716,  0.5020, -0.4107,  1.2846,  2.4297,  0.8886,\n",
      "          0.6491, -2.9506],\n",
      "        [-0.5303,  2.8276, -1.8984, -0.1688, -0.3176,  1.3831,  0.4896, -1.5201,\n",
      "          2.5795, -3.3387],\n",
      "        [ 0.2331, -0.3944,  0.4055, -0.3389,  0.2131, -0.6118, -1.4564, -0.7718,\n",
      "         -0.0443,  1.3823],\n",
      "        [ 0.0687, -1.4636,  0.8006,  0.5560, -0.0539, -0.1743,  1.5414,  2.0590,\n",
      "         -1.7430,  0.5429],\n",
      "        [ 0.3786, -2.0593,  1.3758,  0.1403,  0.2232, -0.9872, -0.2900,  1.1542,\n",
      "         -1.8937,  2.3876],\n",
      "        [ 0.3281,  0.2501,  0.1638, -0.8692,  0.3698, -0.8648, -3.2282, -2.4531,\n",
      "          0.9715,  1.8651],\n",
      "        [-0.1414,  1.0995, -0.6809, -0.2133, -0.0547,  0.3672, -0.3750, -0.9918,\n",
      "          1.1315, -0.9249],\n",
      "        [-0.1916,  0.1972, -0.2691,  0.3404, -0.1862,  0.5034,  1.3828,  0.8498,\n",
      "         -0.1265, -1.1235],\n",
      "        [-0.0755, -0.6325,  0.2530,  0.4799, -0.1349,  0.2015,  1.5838,  1.5402,\n",
      "         -0.9618, -0.3713],\n",
      "        [ 0.4815, -3.0345,  1.9598,  0.3807,  0.2478, -1.2536,  0.2390,  2.1732,\n",
      "         -2.9420,  3.0783]])\n"
     ]
    }
   ],
   "source": [
    "d, k = 10, 10\n",
    "\n",
    "rank = 2\n",
    "W = torch.randn(d,rank) @ torch.randn(rank,k)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that SVD is defined as the following: $$ W = U \\Sigma V^{T}$$ where $W$ is an $m x n$ matrix, $U$ is an $m x m$ matrix, $\\Sigma$ is an m x n matrix and $V^{T}$ is an $n x n$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, V = torch.svd(W)\n",
    "\n",
    "U_r = U[:, :rank]\n",
    "S_r = torch.diag(S[:rank])\n",
    "V_r = V[:, :rank].t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the LoRA paper, recall that we created matrices A (r x k) and B (d x r). We can create these matrices using the rank factorized matrices from the SVD we performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A shape: torch.Size([2, 10])\n",
      "B shape: torch.Size([10, 2])\n"
     ]
    }
   ],
   "source": [
    "A = V_r\n",
    "B = U_r @ S_r\n",
    "\n",
    "print(f\"A shape: {A.shape}\")\n",
    "print(f\"B shape: {B.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test how accurate BA is by comparing how similar the results are when providing input to W and BA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original y using W:\n",
      " tensor([-6.3192, -4.3742,  3.3514, -1.2804,  3.0542,  6.0737, -0.5844, -2.9684,\n",
      "        -2.3654,  3.1847])\n",
      "\n",
      "y' computed using BA:\n",
      " tensor([-6.3192, -4.3742,  3.3514, -1.2804,  3.0542,  6.0737, -0.5844, -2.9684,\n",
      "        -2.3654,  3.1847])\n",
      "Total parameters of W:  100\n",
      "Total parameters of B and A:  40\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(d)\n",
    "\n",
    "# y = Wx\n",
    "y = W @ x\n",
    "# y' = (B*A)x\n",
    "y_prime = (B @ A) @ x\n",
    "\n",
    "print(\"Original y using W:\\n\", y)\n",
    "print(\"\")\n",
    "print(\"y' computed using BA:\\n\", y_prime)\n",
    "\n",
    "print(\"Total parameters of W: \", W.nelement())\n",
    "print(\"Total parameters of B and A: \", B.nelement() + A.nelement())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the basis of why LoRA is such a powerful technique. We were able to replicate the original matrix's behavior with far fewer parameters. This is why LoRA is significantly more memory efficient than other techniques that were created for the purpose of fine-tuning models.\n",
    "\n",
    "# LoRA in application\n",
    "\n",
    "Let's apply LoRA to a simple neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.parametrize as parametrize\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape: torch.Size([64, 1, 28, 28])\n",
      "Label batch shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Download MNIST\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST(root='../data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.MNIST(root='../data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "print(f\"Image batch shape: {images.shape}\")\n",
    "print(f\"Label batch shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple CNN with three convolution layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(Network, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(128 * 4 * 4, 10)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [100/938], Loss: 0.8411\n",
      "Epoch [1/1], Step [200/938], Loss: 0.2673\n",
      "Epoch [1/1], Step [300/938], Loss: 0.1966\n",
      "Epoch [1/1], Step [400/938], Loss: 0.1685\n",
      "Epoch [1/1], Step [500/938], Loss: 0.1367\n",
      "Epoch [1/1], Step [600/938], Loss: 0.1187\n",
      "Epoch [1/1], Step [700/938], Loss: 0.1034\n",
      "Epoch [1/1], Step [800/938], Loss: 0.1004\n",
      "Epoch [1/1], Step [900/938], Loss: 0.0831\n"
     ]
    }
   ],
   "source": [
    "net = Network(input_channels=1)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 1\n",
    "def train():\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print(f'Epoch [{epoch + 1}/{epochs}], Step [{i + 1}/{len(trainloader)}], Loss: {running_loss / 100:.4f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider this to be our \"pre-trained\" weights. We can then apply LoRA on these original weights by fine tuning on a specific number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights shape: 8\n",
      "Layer 1: W: torch.Size([32, 1, 3, 3]) + B: torch.Size([32])\n",
      "Layer 2: W: torch.Size([64, 32, 3, 3]) + B: torch.Size([64])\n",
      "Layer 3: W: torch.Size([128, 64, 3, 3]) + B: torch.Size([128])\n",
      "Total number of parameters: 92,672\n"
     ]
    }
   ],
   "source": [
    "original_weights = {}\n",
    "for name, param in net.named_parameters():\n",
    "    original_weights[name] = param.clone().detach()\n",
    "\n",
    "print(f'Weights shape: {len(original_weights)}')\n",
    "\n",
    "total_parameters_original = 0\n",
    "for index, layer in enumerate([net.conv1, net.conv2, net.conv3]):\n",
    "    total_parameters_original += layer.weight.nelement() + layer.bias.nelement()\n",
    "    print(f'Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape}')\n",
    "print(f'Total number of parameters: {total_parameters_original:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.977\n",
      "wrong counts for the digit 0: 96\n",
      "wrong counts for the digit 1: 208\n",
      "wrong counts for the digit 2: 232\n",
      "wrong counts for the digit 3: 155\n",
      "wrong counts for the digit 4: 126\n",
      "wrong counts for the digit 5: 165\n",
      "wrong counts for the digit 6: 52\n",
      "wrong counts for the digit 7: 69\n",
      "wrong counts for the digit 8: 92\n",
      "wrong counts for the digit 9: 161\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    wrong_counts = [0 for i in range(10)]\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        output = net(inputs)\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == labels[idx]:\n",
    "                correct +=1\n",
    "            else:\n",
    "                wrong_counts[labels[idx]] +=1\n",
    "            total +=1\n",
    "    print(f'Accuracy: {round(correct/total, 3)}')\n",
    "    for i in range(len(wrong_counts)):\n",
    "        print(f'wrong counts for the digit {i}: {wrong_counts[i]}')\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is high number of wrong counts for the number 3, let's fine tune our model on 3. First, we will have to implement LoRA which we can apply to each layer using parametrizations. This implementation follows the original LoRA paper (Hu et. al.). Similar to the paper, I am only applying LoRA to the weight matrix of the convolution layers (excluding the bias). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRA(nn.Module):\n",
    "    def __init__(self, features_in, features_out, rank=1, alpha=1):\n",
    "        super(LoRA, self).__init__()\n",
    "        # A (r x k)\n",
    "        self.lora_A = nn.Parameter(torch.zeros((rank,features_out)))\n",
    "        # B (d x r)\n",
    "        self.lora_B = nn.Parameter(torch.zeros((features_in, rank)))\n",
    "        # Section 4.1 states that A is initialized to a random gaussian distribution while B is intialized to 0. Thus initially, âˆ†W = BA = 0.\n",
    "        nn.init.normal_(self.lora_A, mean=0, std=1)\n",
    "        self.scale = alpha / rank\n",
    "        self.enabled = True\n",
    "\n",
    "    def forward(self, original_weights):\n",
    "        if self.enabled:\n",
    "            # W + BA * scale\n",
    "            return original_weights + torch.matmul(self.lora_B, self.lora_A).view(original_weights.shape) * self.scale\n",
    "        else:\n",
    "            return original_weights\n",
    "\n",
    "def conv_layer_parameterization(layer, rank=1, alpha=1):\n",
    "    out_channels, in_channels, kernel_height, kernel_width = layer.weight.shape\n",
    "    features_in = in_channels * kernel_height * kernel_width\n",
    "    features_out = out_channels\n",
    "    return LoRA(features_in, features_out, rank=rank, alpha=alpha)\n",
    "\n",
    "parametrize.register_parametrization(\n",
    "    net.conv1, \"weight\", conv_layer_parameterization(net.conv1)\n",
    ")\n",
    "parametrize.register_parametrization(\n",
    "    net.conv2, \"weight\", conv_layer_parameterization(net.conv2)\n",
    ")\n",
    "parametrize.register_parametrization(\n",
    "    net.conv3, \"weight\", conv_layer_parameterization(net.conv3)\n",
    ")\n",
    "\n",
    "def enable_disable_lora(enabled=True):\n",
    "    for layer in [net.conv1, net.conv2, net.conv3]:\n",
    "        layer.parametrizations[\"weight\"][0].enabled = enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: W: torch.Size([32, 1, 3, 3]) + B: torch.Size([32]) + Lora_A: torch.Size([1, 32]) + Lora_B: torch.Size([9, 1])\n",
      "Layer 2: W: torch.Size([64, 32, 3, 3]) + B: torch.Size([64]) + Lora_A: torch.Size([1, 64]) + Lora_B: torch.Size([288, 1])\n",
      "Layer 3: W: torch.Size([128, 64, 3, 3]) + B: torch.Size([128]) + Lora_A: torch.Size([1, 128]) + Lora_B: torch.Size([576, 1])\n",
      "Total number of parameters (original): 92,672\n",
      "Total number of parameters (original + LoRA): 93,769\n",
      "Parameters introduced by LoRA: 1,097\n",
      "Parameters increment: 1.184%\n"
     ]
    }
   ],
   "source": [
    "total_parameters_lora = 0\n",
    "total_parameters_non_lora = 0\n",
    "for index, layer in enumerate([net.conv1, net.conv2, net.conv3]):\n",
    "    total_parameters_lora += layer.parametrizations[\"weight\"][0].lora_A.nelement() + layer.parametrizations[\"weight\"][0].lora_B.nelement()\n",
    "    total_parameters_non_lora += layer.weight.nelement() + layer.bias.nelement()\n",
    "    print(\n",
    "        f'Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape} + Lora_A: {layer.parametrizations[\"weight\"][0].lora_A.shape} + Lora_B: {layer.parametrizations[\"weight\"][0].lora_B.shape}'\n",
    "    )\n",
    "\n",
    "assert total_parameters_non_lora == total_parameters_original\n",
    "\n",
    "print(f'Total number of parameters (original): {total_parameters_non_lora:,}')\n",
    "print(f'Total number of parameters (original + LoRA): {total_parameters_lora + total_parameters_non_lora:,}')\n",
    "print(f'Parameters introduced by LoRA: {total_parameters_lora:,}')\n",
    "parameters_increment = (total_parameters_lora / total_parameters_non_lora) * 100\n",
    "print(f'Parameters increment: {parameters_increment:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we did not have LoRA, we would have had double the number of parameters as the original as we would have needed a new weight value for each parameter. However, with this approach, we are only adding in a marginal fraction of the original number of parameters, 1.184% to be exact.\n",
    "\n",
    "Now, we can fine-tune our model by training specifically on the data with label = 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing non-LoRA parameter conv1.bias\n",
      "Freezing non-LoRA parameter conv1.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter conv2.bias\n",
      "Freezing non-LoRA parameter conv2.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter conv3.bias\n",
      "Freezing non-LoRA parameter conv3.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter fc.weight\n",
      "Freezing non-LoRA parameter fc.bias\n",
      "Epoch [1/1], Step [100/938], Loss: 0.0713\n",
      "Epoch [1/1], Step [200/938], Loss: 0.0823\n",
      "Epoch [1/1], Step [300/938], Loss: 0.0889\n",
      "Epoch [1/1], Step [400/938], Loss: 0.0779\n",
      "Epoch [1/1], Step [500/938], Loss: 0.0789\n",
      "Epoch [1/1], Step [600/938], Loss: 0.0774\n",
      "Epoch [1/1], Step [700/938], Loss: 0.0860\n",
      "Epoch [1/1], Step [800/938], Loss: 0.0732\n",
      "Epoch [1/1], Step [900/938], Loss: 0.0762\n"
     ]
    }
   ],
   "source": [
    "# Freeze the non-Lora parameters\n",
    "for name, param in net.named_parameters():\n",
    "    if 'lora' not in name:\n",
    "        print(f'Freezing non-LoRA parameter {name}')\n",
    "        param.requires_grad = False\n",
    "\n",
    "trainset = datasets.MNIST(root='../data', train=True, download=True, transform=transform)\n",
    "\n",
    "label_3_indices = trainset.targets == 3\n",
    "label_3_data = trainset.data[label_3_indices]\n",
    "label_3_targets = trainset.targets[label_3_indices]\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Conv1 Weights ---\n",
      "tensor([[[[-0.3201,  0.2351, -0.0387],\n",
      "          [ 0.2227,  0.1490,  0.0261],\n",
      "          [ 0.1478,  0.1012,  0.1695]]],\n",
      "\n",
      "\n",
      "        [[[-0.1382,  0.0292,  0.0362],\n",
      "          [ 0.0689,  0.0742,  0.3690],\n",
      "          [ 0.0837, -0.1061, -0.1688]]],\n",
      "\n",
      "\n",
      "        [[[-0.0426, -0.0773, -0.0327],\n",
      "          [ 0.0630,  0.3050,  0.2703],\n",
      "          [-0.2952,  0.2926,  0.1694]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3924,  0.3351, -0.2210],\n",
      "          [-0.3755, -0.1339,  0.3505],\n",
      "          [-0.1859,  0.0849, -0.1109]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3868, -0.0481,  0.3359],\n",
      "          [ 0.0497, -0.0952,  0.2387],\n",
      "          [-0.1587,  0.1403, -0.0474]]],\n",
      "\n",
      "\n",
      "        [[[-0.0246, -0.3671, -0.1821],\n",
      "          [ 0.2409, -0.0310,  0.3586],\n",
      "          [ 0.3353,  0.0215, -0.2283]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2614,  0.1904, -0.1782],\n",
      "          [ 0.2923,  0.3085,  0.3480],\n",
      "          [-0.1351,  0.0121,  0.0247]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0540, -0.3455, -0.3605],\n",
      "          [ 0.2408, -0.0068,  0.1748],\n",
      "          [ 0.0752,  0.2101,  0.1340]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2345,  0.1978, -0.4171],\n",
      "          [ 0.3029, -0.1583, -0.3438],\n",
      "          [ 0.1323, -0.0091,  0.1361]]],\n",
      "\n",
      "\n",
      "        [[[-0.3050, -0.2567,  0.3528],\n",
      "          [-0.1016,  0.2573,  0.2394],\n",
      "          [-0.2459,  0.1562,  0.2072]]],\n",
      "\n",
      "\n",
      "        [[[-0.3655, -0.2818,  0.0165],\n",
      "          [ 0.0325, -0.1323, -0.0758],\n",
      "          [ 0.3860,  0.0545,  0.2865]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1952, -0.1454, -0.2337],\n",
      "          [ 0.3829,  0.1071,  0.2851],\n",
      "          [ 0.1293,  0.1774,  0.2932]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4025, -0.1134,  0.1182],\n",
      "          [ 0.3334,  0.3381,  0.2703],\n",
      "          [-0.1981, -0.1956, -0.1659]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0767,  0.3695, -0.2520],\n",
      "          [ 0.2215, -0.1211, -0.3393],\n",
      "          [-0.0399,  0.2465,  0.3220]]],\n",
      "\n",
      "\n",
      "        [[[-0.1735,  0.1564,  0.3521],\n",
      "          [ 0.2921,  0.3238,  0.3213],\n",
      "          [ 0.2660,  0.1743, -0.1692]]],\n",
      "\n",
      "\n",
      "        [[[-0.3860, -0.3190, -0.1633],\n",
      "          [-0.2640,  0.0310,  0.0450],\n",
      "          [-0.0767,  0.1233,  0.1358]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3069,  0.3557,  0.0151],\n",
      "          [ 0.3907,  0.0407,  0.0635],\n",
      "          [-0.2868, -0.1922, -0.1517]]],\n",
      "\n",
      "\n",
      "        [[[-0.1567,  0.2823, -0.0597],\n",
      "          [-0.2294,  0.2029,  0.1658],\n",
      "          [ 0.2319, -0.1251,  0.1102]]],\n",
      "\n",
      "\n",
      "        [[[-0.1286, -0.0439, -0.1617],\n",
      "          [-0.1317, -0.0079,  0.2689],\n",
      "          [ 0.1540,  0.1974,  0.4419]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4099,  0.3406,  0.1054],\n",
      "          [-0.1723,  0.3472, -0.0201],\n",
      "          [-0.0032, -0.2449, -0.0141]]],\n",
      "\n",
      "\n",
      "        [[[-0.2150,  0.3063, -0.2882],\n",
      "          [-0.2598, -0.0113, -0.1802],\n",
      "          [ 0.1193, -0.1283, -0.1566]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0897,  0.3479, -0.1693],\n",
      "          [ 0.2530,  0.3478,  0.4101],\n",
      "          [-0.1765,  0.1603, -0.1969]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3192, -0.0714,  0.2459],\n",
      "          [ 0.0529,  0.0496,  0.0401],\n",
      "          [ 0.0800,  0.2022,  0.0814]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1120, -0.2262,  0.3222],\n",
      "          [ 0.3458,  0.3871,  0.3416],\n",
      "          [-0.2163, -0.0413,  0.3633]]],\n",
      "\n",
      "\n",
      "        [[[-0.2437,  0.3753, -0.1941],\n",
      "          [-0.2169,  0.2866, -0.1592],\n",
      "          [-0.0275,  0.3711,  0.3447]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3786, -0.0592, -0.3144],\n",
      "          [ 0.3925, -0.0426, -0.1762],\n",
      "          [ 0.1317, -0.2277,  0.1737]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2157, -0.2510, -0.3470],\n",
      "          [-0.0563,  0.0214, -0.1523],\n",
      "          [-0.2284, -0.1217, -0.3622]]],\n",
      "\n",
      "\n",
      "        [[[-0.1891, -0.1449,  0.3306],\n",
      "          [ 0.3836,  0.0103,  0.0998],\n",
      "          [ 0.2289,  0.1379, -0.1178]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2361,  0.2774,  0.0394],\n",
      "          [ 0.0181,  0.4025,  0.3619],\n",
      "          [-0.1500,  0.3928,  0.1251]]],\n",
      "\n",
      "\n",
      "        [[[-0.1816,  0.2494, -0.1914],\n",
      "          [ 0.3760, -0.1161,  0.1639],\n",
      "          [ 0.1695,  0.1614,  0.3292]]],\n",
      "\n",
      "\n",
      "        [[[-0.1543, -0.2643, -0.3664],\n",
      "          [ 0.0640, -0.0985,  0.0626],\n",
      "          [ 0.0955,  0.2638,  0.3955]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2061,  0.0280, -0.1798],\n",
      "          [-0.1616,  0.4772, -0.0812],\n",
      "          [ 0.1350,  0.1497, -0.2400]]]], grad_fn=<AddBackward0>)\n",
      "--- Conv1 LoRA A Weights ---\n",
      "Parameter containing:\n",
      "tensor([[ 1.2795e+00,  1.2405e+00, -6.3524e-01,  1.2796e+00, -8.9946e-01,\n",
      "         -8.0545e-01, -2.7611e-01,  9.5978e-01, -1.1070e+00,  4.9953e-01,\n",
      "          5.7256e-01, -1.7117e+00,  5.2710e-01,  6.4099e-01,  1.6806e-01,\n",
      "         -8.1721e-04, -2.2800e+00,  8.4235e-01,  9.0470e-01, -2.0053e+00,\n",
      "         -4.9219e-02, -8.2659e-01, -1.4605e-01, -1.6812e-01,  4.4979e-01,\n",
      "         -7.2448e-02,  1.4574e+00, -1.3811e+00,  6.1233e-01,  6.7463e-01,\n",
      "         -1.1012e+00, -7.3082e-01]], requires_grad=True)\n",
      "--- Conv1 LoRA B Weights ---\n",
      "Parameter containing:\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], requires_grad=True)\n",
      "\n",
      "\n",
      "--- Conv1 with LoRA Enabled Weights ---\n",
      "tensor([[[[-0.3201,  0.2351, -0.0387],\n",
      "          [ 0.2227,  0.1490,  0.0261],\n",
      "          [ 0.1478,  0.1012,  0.1695]]],\n",
      "\n",
      "\n",
      "        [[[-0.1382,  0.0292,  0.0362],\n",
      "          [ 0.0689,  0.0742,  0.3690],\n",
      "          [ 0.0837, -0.1061, -0.1688]]],\n",
      "\n",
      "\n",
      "        [[[-0.0426, -0.0773, -0.0327],\n",
      "          [ 0.0630,  0.3050,  0.2703],\n",
      "          [-0.2952,  0.2926,  0.1694]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3924,  0.3351, -0.2210],\n",
      "          [-0.3755, -0.1339,  0.3505],\n",
      "          [-0.1859,  0.0849, -0.1109]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3868, -0.0481,  0.3359],\n",
      "          [ 0.0497, -0.0952,  0.2387],\n",
      "          [-0.1587,  0.1403, -0.0474]]],\n",
      "\n",
      "\n",
      "        [[[-0.0246, -0.3671, -0.1821],\n",
      "          [ 0.2409, -0.0310,  0.3586],\n",
      "          [ 0.3353,  0.0215, -0.2283]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2614,  0.1904, -0.1782],\n",
      "          [ 0.2923,  0.3085,  0.3480],\n",
      "          [-0.1351,  0.0121,  0.0247]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0540, -0.3455, -0.3605],\n",
      "          [ 0.2408, -0.0068,  0.1748],\n",
      "          [ 0.0752,  0.2101,  0.1340]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2345,  0.1978, -0.4171],\n",
      "          [ 0.3029, -0.1583, -0.3438],\n",
      "          [ 0.1323, -0.0091,  0.1361]]],\n",
      "\n",
      "\n",
      "        [[[-0.3050, -0.2567,  0.3528],\n",
      "          [-0.1016,  0.2573,  0.2394],\n",
      "          [-0.2459,  0.1562,  0.2072]]],\n",
      "\n",
      "\n",
      "        [[[-0.3655, -0.2818,  0.0165],\n",
      "          [ 0.0325, -0.1323, -0.0758],\n",
      "          [ 0.3860,  0.0545,  0.2865]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1952, -0.1454, -0.2337],\n",
      "          [ 0.3829,  0.1071,  0.2851],\n",
      "          [ 0.1293,  0.1774,  0.2932]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4025, -0.1134,  0.1182],\n",
      "          [ 0.3334,  0.3381,  0.2703],\n",
      "          [-0.1981, -0.1956, -0.1659]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0767,  0.3695, -0.2520],\n",
      "          [ 0.2215, -0.1211, -0.3393],\n",
      "          [-0.0399,  0.2465,  0.3220]]],\n",
      "\n",
      "\n",
      "        [[[-0.1735,  0.1564,  0.3521],\n",
      "          [ 0.2921,  0.3238,  0.3213],\n",
      "          [ 0.2660,  0.1743, -0.1692]]],\n",
      "\n",
      "\n",
      "        [[[-0.3860, -0.3190, -0.1633],\n",
      "          [-0.2640,  0.0310,  0.0450],\n",
      "          [-0.0767,  0.1233,  0.1358]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3069,  0.3557,  0.0151],\n",
      "          [ 0.3907,  0.0407,  0.0635],\n",
      "          [-0.2868, -0.1922, -0.1517]]],\n",
      "\n",
      "\n",
      "        [[[-0.1567,  0.2823, -0.0597],\n",
      "          [-0.2294,  0.2029,  0.1658],\n",
      "          [ 0.2319, -0.1251,  0.1102]]],\n",
      "\n",
      "\n",
      "        [[[-0.1286, -0.0439, -0.1617],\n",
      "          [-0.1317, -0.0079,  0.2689],\n",
      "          [ 0.1540,  0.1974,  0.4419]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4099,  0.3406,  0.1054],\n",
      "          [-0.1723,  0.3472, -0.0201],\n",
      "          [-0.0032, -0.2449, -0.0141]]],\n",
      "\n",
      "\n",
      "        [[[-0.2150,  0.3063, -0.2882],\n",
      "          [-0.2598, -0.0113, -0.1802],\n",
      "          [ 0.1193, -0.1283, -0.1566]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0897,  0.3479, -0.1693],\n",
      "          [ 0.2530,  0.3478,  0.4101],\n",
      "          [-0.1765,  0.1603, -0.1969]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3192, -0.0714,  0.2459],\n",
      "          [ 0.0529,  0.0496,  0.0401],\n",
      "          [ 0.0800,  0.2022,  0.0814]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1120, -0.2262,  0.3222],\n",
      "          [ 0.3458,  0.3871,  0.3416],\n",
      "          [-0.2163, -0.0413,  0.3633]]],\n",
      "\n",
      "\n",
      "        [[[-0.2437,  0.3753, -0.1941],\n",
      "          [-0.2169,  0.2866, -0.1592],\n",
      "          [-0.0275,  0.3711,  0.3447]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3786, -0.0592, -0.3144],\n",
      "          [ 0.3925, -0.0426, -0.1762],\n",
      "          [ 0.1317, -0.2277,  0.1737]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2157, -0.2510, -0.3470],\n",
      "          [-0.0563,  0.0214, -0.1523],\n",
      "          [-0.2284, -0.1217, -0.3622]]],\n",
      "\n",
      "\n",
      "        [[[-0.1891, -0.1449,  0.3306],\n",
      "          [ 0.3836,  0.0103,  0.0998],\n",
      "          [ 0.2289,  0.1379, -0.1178]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2361,  0.2774,  0.0394],\n",
      "          [ 0.0181,  0.4025,  0.3619],\n",
      "          [-0.1500,  0.3928,  0.1251]]],\n",
      "\n",
      "\n",
      "        [[[-0.1816,  0.2494, -0.1914],\n",
      "          [ 0.3760, -0.1161,  0.1639],\n",
      "          [ 0.1695,  0.1614,  0.3292]]],\n",
      "\n",
      "\n",
      "        [[[-0.1543, -0.2643, -0.3664],\n",
      "          [ 0.0640, -0.0985,  0.0626],\n",
      "          [ 0.0955,  0.2638,  0.3955]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2061,  0.0280, -0.1798],\n",
      "          [-0.1616,  0.4772, -0.0812],\n",
      "          [ 0.1350,  0.1497, -0.2400]]]], grad_fn=<AddBackward0>)\n",
      "--- Conv1 with LoRA Enabled LoRA A Weights ---\n",
      "Parameter containing:\n",
      "tensor([[ 1.2795e+00,  1.2405e+00, -6.3524e-01,  1.2796e+00, -8.9946e-01,\n",
      "         -8.0545e-01, -2.7611e-01,  9.5978e-01, -1.1070e+00,  4.9953e-01,\n",
      "          5.7256e-01, -1.7117e+00,  5.2710e-01,  6.4099e-01,  1.6806e-01,\n",
      "         -8.1721e-04, -2.2800e+00,  8.4235e-01,  9.0470e-01, -2.0053e+00,\n",
      "         -4.9219e-02, -8.2659e-01, -1.4605e-01, -1.6812e-01,  4.4979e-01,\n",
      "         -7.2448e-02,  1.4574e+00, -1.3811e+00,  6.1233e-01,  6.7463e-01,\n",
      "         -1.1012e+00, -7.3082e-01]], requires_grad=True)\n",
      "--- Conv1 with LoRA Enabled LoRA B Weights ---\n",
      "Parameter containing:\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], requires_grad=True)\n",
      "\n",
      "\n",
      "--- Conv1 with LoRA Disabled Weights ---\n",
      "Parameter containing:\n",
      "tensor([[[[-0.3201,  0.2351, -0.0387],\n",
      "          [ 0.2227,  0.1490,  0.0261],\n",
      "          [ 0.1478,  0.1012,  0.1695]]],\n",
      "\n",
      "\n",
      "        [[[-0.1382,  0.0292,  0.0362],\n",
      "          [ 0.0689,  0.0742,  0.3690],\n",
      "          [ 0.0837, -0.1061, -0.1688]]],\n",
      "\n",
      "\n",
      "        [[[-0.0426, -0.0773, -0.0327],\n",
      "          [ 0.0630,  0.3050,  0.2703],\n",
      "          [-0.2952,  0.2926,  0.1694]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3924,  0.3351, -0.2210],\n",
      "          [-0.3755, -0.1339,  0.3505],\n",
      "          [-0.1859,  0.0849, -0.1109]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3868, -0.0481,  0.3359],\n",
      "          [ 0.0497, -0.0952,  0.2387],\n",
      "          [-0.1587,  0.1403, -0.0474]]],\n",
      "\n",
      "\n",
      "        [[[-0.0246, -0.3671, -0.1821],\n",
      "          [ 0.2409, -0.0310,  0.3586],\n",
      "          [ 0.3353,  0.0215, -0.2283]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2614,  0.1904, -0.1782],\n",
      "          [ 0.2923,  0.3085,  0.3480],\n",
      "          [-0.1351,  0.0121,  0.0247]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0540, -0.3455, -0.3605],\n",
      "          [ 0.2408, -0.0068,  0.1748],\n",
      "          [ 0.0752,  0.2101,  0.1340]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2345,  0.1978, -0.4171],\n",
      "          [ 0.3029, -0.1583, -0.3438],\n",
      "          [ 0.1323, -0.0091,  0.1361]]],\n",
      "\n",
      "\n",
      "        [[[-0.3050, -0.2567,  0.3528],\n",
      "          [-0.1016,  0.2573,  0.2394],\n",
      "          [-0.2459,  0.1562,  0.2072]]],\n",
      "\n",
      "\n",
      "        [[[-0.3655, -0.2818,  0.0165],\n",
      "          [ 0.0325, -0.1323, -0.0758],\n",
      "          [ 0.3860,  0.0545,  0.2865]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1952, -0.1454, -0.2337],\n",
      "          [ 0.3829,  0.1071,  0.2851],\n",
      "          [ 0.1293,  0.1774,  0.2932]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4025, -0.1134,  0.1182],\n",
      "          [ 0.3334,  0.3381,  0.2703],\n",
      "          [-0.1981, -0.1956, -0.1659]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0767,  0.3695, -0.2520],\n",
      "          [ 0.2215, -0.1211, -0.3393],\n",
      "          [-0.0399,  0.2465,  0.3220]]],\n",
      "\n",
      "\n",
      "        [[[-0.1735,  0.1564,  0.3521],\n",
      "          [ 0.2921,  0.3238,  0.3213],\n",
      "          [ 0.2660,  0.1743, -0.1692]]],\n",
      "\n",
      "\n",
      "        [[[-0.3860, -0.3190, -0.1633],\n",
      "          [-0.2640,  0.0310,  0.0450],\n",
      "          [-0.0767,  0.1233,  0.1358]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3069,  0.3557,  0.0151],\n",
      "          [ 0.3907,  0.0407,  0.0635],\n",
      "          [-0.2868, -0.1922, -0.1517]]],\n",
      "\n",
      "\n",
      "        [[[-0.1567,  0.2823, -0.0597],\n",
      "          [-0.2294,  0.2029,  0.1658],\n",
      "          [ 0.2319, -0.1251,  0.1102]]],\n",
      "\n",
      "\n",
      "        [[[-0.1286, -0.0439, -0.1617],\n",
      "          [-0.1317, -0.0079,  0.2689],\n",
      "          [ 0.1540,  0.1974,  0.4419]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4099,  0.3406,  0.1054],\n",
      "          [-0.1723,  0.3472, -0.0201],\n",
      "          [-0.0032, -0.2449, -0.0141]]],\n",
      "\n",
      "\n",
      "        [[[-0.2150,  0.3063, -0.2882],\n",
      "          [-0.2598, -0.0113, -0.1802],\n",
      "          [ 0.1193, -0.1283, -0.1566]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0897,  0.3479, -0.1693],\n",
      "          [ 0.2530,  0.3478,  0.4101],\n",
      "          [-0.1765,  0.1603, -0.1969]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3192, -0.0714,  0.2459],\n",
      "          [ 0.0529,  0.0496,  0.0401],\n",
      "          [ 0.0800,  0.2022,  0.0814]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1120, -0.2262,  0.3222],\n",
      "          [ 0.3458,  0.3871,  0.3416],\n",
      "          [-0.2163, -0.0413,  0.3633]]],\n",
      "\n",
      "\n",
      "        [[[-0.2437,  0.3753, -0.1941],\n",
      "          [-0.2169,  0.2866, -0.1592],\n",
      "          [-0.0275,  0.3711,  0.3447]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3786, -0.0592, -0.3144],\n",
      "          [ 0.3925, -0.0426, -0.1762],\n",
      "          [ 0.1317, -0.2277,  0.1737]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2157, -0.2510, -0.3470],\n",
      "          [-0.0563,  0.0214, -0.1523],\n",
      "          [-0.2284, -0.1217, -0.3622]]],\n",
      "\n",
      "\n",
      "        [[[-0.1891, -0.1449,  0.3306],\n",
      "          [ 0.3836,  0.0103,  0.0998],\n",
      "          [ 0.2289,  0.1379, -0.1178]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2361,  0.2774,  0.0394],\n",
      "          [ 0.0181,  0.4025,  0.3619],\n",
      "          [-0.1500,  0.3928,  0.1251]]],\n",
      "\n",
      "\n",
      "        [[[-0.1816,  0.2494, -0.1914],\n",
      "          [ 0.3760, -0.1161,  0.1639],\n",
      "          [ 0.1695,  0.1614,  0.3292]]],\n",
      "\n",
      "\n",
      "        [[[-0.1543, -0.2643, -0.3664],\n",
      "          [ 0.0640, -0.0985,  0.0626],\n",
      "          [ 0.0955,  0.2638,  0.3955]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2061,  0.0280, -0.1798],\n",
      "          [-0.1616,  0.4772, -0.0812],\n",
      "          [ 0.1350,  0.1497, -0.2400]]]])\n",
      "--- Conv1 with LoRA Disabled LoRA A Weights ---\n",
      "Parameter containing:\n",
      "tensor([[ 1.2795e+00,  1.2405e+00, -6.3524e-01,  1.2796e+00, -8.9946e-01,\n",
      "         -8.0545e-01, -2.7611e-01,  9.5978e-01, -1.1070e+00,  4.9953e-01,\n",
      "          5.7256e-01, -1.7117e+00,  5.2710e-01,  6.4099e-01,  1.6806e-01,\n",
      "         -8.1721e-04, -2.2800e+00,  8.4235e-01,  9.0470e-01, -2.0053e+00,\n",
      "         -4.9219e-02, -8.2659e-01, -1.4605e-01, -1.6812e-01,  4.4979e-01,\n",
      "         -7.2448e-02,  1.4574e+00, -1.3811e+00,  6.1233e-01,  6.7463e-01,\n",
      "         -1.1012e+00, -7.3082e-01]], requires_grad=True)\n",
      "--- Conv1 with LoRA Disabled LoRA B Weights ---\n",
      "Parameter containing:\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], requires_grad=True)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_layer_weights(layer, name):\n",
    "    print(f\"--- {name} Weights ---\")\n",
    "    print(layer.weight)\n",
    "    if \"weight\" in layer.parametrizations:\n",
    "        print(f\"--- {name} LoRA A Weights ---\")\n",
    "        print(layer.parametrizations[\"weight\"][0].lora_A)\n",
    "        print(f\"--- {name} LoRA B Weights ---\")\n",
    "        print(layer.parametrizations[\"weight\"][0].lora_B)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "print_layer_weights(net.conv1, \"Conv1\")\n",
    "enable_disable_lora(True)\n",
    "print_layer_weights(net.conv1, \"Conv1 with LoRA Enabled\")\n",
    "enable_disable_lora(False)\n",
    "print_layer_weights(net.conv1, \"Conv1 with LoRA Disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test our fine-tuned network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.977\n",
      "wrong counts for the digit 0: 96\n",
      "wrong counts for the digit 1: 208\n",
      "wrong counts for the digit 2: 232\n",
      "wrong counts for the digit 3: 155\n",
      "wrong counts for the digit 4: 126\n",
      "wrong counts for the digit 5: 165\n",
      "wrong counts for the digit 6: 52\n",
      "wrong counts for the digit 7: 69\n",
      "wrong counts for the digit 8: 92\n",
      "wrong counts for the digit 9: 161\n"
     ]
    }
   ],
   "source": [
    "enable_disable_lora(enabled=True)\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.977\n",
      "wrong counts for the digit 0: 96\n",
      "wrong counts for the digit 1: 208\n",
      "wrong counts for the digit 2: 232\n",
      "wrong counts for the digit 3: 155\n",
      "wrong counts for the digit 4: 126\n",
      "wrong counts for the digit 5: 165\n",
      "wrong counts for the digit 6: 52\n",
      "wrong counts for the digit 7: 69\n",
      "wrong counts for the digit 8: 92\n",
      "wrong counts for the digit 9: 161\n"
     ]
    }
   ],
   "source": [
    "enable_disable_lora(enabled=False)\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
